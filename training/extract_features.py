import binary2strings as b2s
import argparse
import sys
import os
import csv
import subprocess
import hashlib
from joblib import Parallel, delayed
from tqdm import tqdm
from dataclasses import dataclass, field
import lief

TO_IGNORE = [".gitkeep"]
INCLUDE_TYPES = ["PE32", "ELF"]


@dataclass
class Features:
    sha256: str = ""
    file_type: str = ""
    file_size: int = 0
    compilation_date: int = 0
    machine: str = ""
    number_of_sections: int = 0
    number_of_strings: int = 0
    hashed_strings: list = field(default_factory=list)

    def to_row(self):
        return [
            self.sha256,
            self.file_type,
            self.file_size,
            self.compilation_date,
            self.machine,
            self.number_of_sections,
            self.number_of_strings,
        ] + [s for s in self.hashed_strings]


def extract_strings(path: str, only_interesting: bool = True):
    with open(path, "rb") as f:
        res = b2s.extract_all_strings(
            f.read(), min_chars=4, only_interesting=only_interesting
        )
    return [s for s, *_ in res]


def get_file_from_magic(path: str) -> str:
    # use the file command on linux to get info on the file
    out = subprocess.check_output(f"file {path}", shell=True)
    return out.decode().strip().split(path + ": ")[1]


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--input",
        help="input folder with binary files to extract features from",
        default="./samples",
        nargs="?",
    )
    parser.add_argument(
        "-o",
        "--output",
        help="output csv file with the features",
        default="./malware-features.csv",
        nargs="?",
    )

    return parser.parse_args()


def get_files_to_analyse(args: argparse.Namespace):
    print("[^] Listing files to analyse")

    def process(file: str) -> str | None:
        # check if the file needs to be ignored
        if file in TO_IGNORE:
            return None

        # check if the file is of the right type
        f_type = get_file_from_magic(os.path.join(args.input, file))
        for it in INCLUDE_TYPES:
            if it in f_type:
                break
        else:
            # not a type we know
            return None

        # this is a file we want
        return file

    to_analyse = Parallel(n_jobs=os.cpu_count())(
        delayed(process)(file)
        for file in tqdm(os.listdir(args.input), desc="Processing samples")
    )
    to_analyse = [f for f in to_analyse if f is not None]

    print(f"[+] {len(to_analyse)} files to analyse")
    return to_analyse


def get_features(path: str) -> Features:
    # compute file hash
    with open(path, "rb") as f:
        sha256 = hashlib.sha256(f.read()).hexdigest()

    # get file type
    f_magic = get_file_from_magic(path)
    if "ELF" in f_magic:
        # lets work with an ELF file
        elf = lief.ELF.parse(path)

        machine = elf.header.machine_type.name.lower()
        file_type = elf.header.file_type.name.lower()
        nb_sections = len(elf.sections)
        compilation_date = 0  # TODO: find some kind of date to add

        del elf
    elif "PE32" in f_magic or "PE64" in f_magic:
        # lets work with a PE file
        pe = lief.PE.parse(path)

        machine = pe.header.machine.name.lower()
        file_type = (
            "executable"
            if pe.header.has_characteristic(
                lief.PE.HEADER_CHARACTERISTICS.EXECUTABLE_IMAGE
            )
            else "library"
        )
        nb_sections = pe.header.numberof_sections
        compilation_date = pe.header.time_date_stamps

        del pe

    # work with extracted strings
    strings = extract_strings(path, False)
    nb_strings = len(strings)

    # hash the strings
    hashed = [0 for _ in range(2000)]
    for s in strings:
        hash = hashlib.sha256(s.encode()).hexdigest()
        hashed[int(hash, 16) % 2000] += 1
    del strings

    return Features(
        sha256=sha256,
        file_type=file_type,
        file_size=os.stat(path).st_size,
        compilation_date=compilation_date,
        machine=machine,
        number_of_sections=nb_sections,
        number_of_strings=nb_strings,
        hashed_strings=hashed,
    )


def init_csv(path: str):
    header = [
        "sha256",
        "file_type",
        "file_size",
        "compilation_date",
        "machine",
        "number_of_sections",
        "number_of_strings",
    ] + [f"s_{k}" for k in range(2000)]

    print("[^] Initializing CSV file with features")
    with open(path, "w") as f:
        writer = csv.writer(f)
        writer.writerow(header)


def save_features_to_csv(feat: Features, path: str):
    with open(path, "a") as f:
        writer = csv.writer(f)
        writer.writerow(feat.to_row())


def main():
    args = parse_args()

    # get the list of samples to analyse
    to_analyse = get_files_to_analyse(args)

    # init the csv file holding all the data
    init_csv(args.output)

    # run the pipeline on the files
    def pipeline(file: str):
        feat = get_features(file)
        save_features_to_csv(feat, args.output)

    Parallel(n_jobs=os.cpu_count())(
        delayed(pipeline)(os.path.join(args.input, file))
        for file in tqdm(to_analyse, desc="Extracting features")
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
