import sys
import argparse
import requests
import tqdm
import os
import zipfile
from bs4 import BeautifulSoup
from joblib import Parallel, delayed
import multiprocessing

URL = "https://datalake.abuse.ch/malware-bazaar/daily/"


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-o",
        "--output",
        help="folder to which download samples",
        type=str,
        default="./samples",
        nargs="?",
    )
    parser.add_argument(
        "-e",
        "--extract",
        help="should the archives be extracted",
        type=bool,
        default=False,
        nargs="?",
    )
    parser.add_argument(
        "nb", help="number of batches to download", type=int, default=5, nargs="?"
    )

    return parser.parse_args()


def get_daily_batches():
    dates = []

    # make the request
    body = requests.get(URL).content

    # parse the content to get the list of available samples
    soup = BeautifulSoup(body, "lxml")
    table = soup.find("table")
    for row in table.findAll("a"):
        # keep only the dates
        if row.text.count("-") != 2:
            continue

        dates.append(row.text.replace(".zip", ""))

    return sorted(dates, reverse=True)


def download_batch(index: int, name: str, output: str, should_extract: bool):
    # get worker id
    id = multiprocessing.current_process()._identity[0]

    # download as a stream to get the progress bar
    res = requests.get(URL + name + ".zip", stream=True)

    # get file size
    total_length = int(res.headers.get("content-length"))

    archive_name = os.path.join(output, name + ".zip")

    # download the file
    with open(archive_name, "wb") as fh, tqdm.tqdm(
        desc=f"[{index}] {name}.zip",
        total=total_length,
        unit="iB",
        unit_scale=True,
        unit_divisor=1024,
        position=id - 1,
    ) as bar:
        for data in res.iter_content(chunk_size=4096):
            bar.update(len(data))
            fh.write(data)

    # extract it
    if should_extract:
        print(f"Extracting {name}.zip")
        with zipfile.ZipFile(archive_name) as zf:
            zf.extractall(pwd=b"infected", path=output)

        # delete the file once extracted
        os.remove(archive_name)


def main():
    # parse arguments
    args = parse_args()
    print(f"Running with should_extract = {args.extract}")

    # get the n latest batches
    batches_to_download = get_daily_batches()[: args.nb]

    def process_batch(index: int, name: str):
        # download and extract samples
        download_batch(index, name, args.output, args.extract)

    Parallel(n_jobs=os.cpu_count())(
        delayed(process_batch)(idx, name)
        for idx, name in enumerate(batches_to_download)
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
