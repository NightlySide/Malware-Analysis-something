import sys
import argparse
import requests
import tqdm
import os
import zipfile
from bs4 import BeautifulSoup
from joblib import Parallel, delayed

URL = "https://datalake.abuse.ch/malware-bazaar/daily/"

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-o", "--output", help="folder to which download samples", type=str, default="./samples", nargs="?")
    parser.add_argument("-e", "--extract", help="should the archives be extracted", type=bool, default=True, nargs="?")
    parser.add_argument("nb", help="number of batches to download", type=int, default=5, nargs="?")
    
    return parser.parse_args()

def get_daily_batches():
    dates = []
    
    # make the request
    body = requests.get(URL).content
    
    # parse the content to get the list of available samples
    soup = BeautifulSoup(body, "lxml")
    table = soup.find("table")
    for row in table.findAll("a"):
        # keep only the dates
        if row.text.count("-") != 2:
            continue
        
        dates.append(row.text.replace(".zip", ""))
        
    return sorted(dates, reverse=True)

def download_batch(name: str, output: str, should_extract: bool):
    # download as a stream to get the progress bar
    res = requests.get(URL + name + ".zip", stream=True)
    
    # get file size
    total_length = int(res.headers.get('content-length'))
    
    archive_name = os.path.join(output, name + ".zip")
    
    #Â download the file
    with open(archive_name, "wb") as fh, tqdm.tqdm(
        desc=name + ".zip",
        total=total_length,
        unit='iB',
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for data in res.iter_content(chunk_size=4096):
            bar.update(len(data))
            fh.write(data)
            
            
    # extract it
    if should_extract:
        print(f"Extracting {name}.zip")
        with zipfile.ZipFile(archive_name) as zf:
            zf.extractall(pwd=b"infected", path=output)
            
        # delete the file once extracted
        os.remove(archive_name)
    
def main():
    # parse arguments
    args = parse_args()
    
    # get the n latest batches
    batches_to_download = get_daily_batches()[:args.nb]

    def process_batch(index: int, name: str):
        # download and extract samples
        print(f"Downloading batch: {index}")
        download_batch(name, args.output, args.extract)
        
    Parallel(n_jobs=os.cpu_count())(delayed(process_batch)(idx, name) for idx, name in enumerate(batches_to_download))
    
    return 0
    
if __name__ == "__main__":
    sys.exit(main())